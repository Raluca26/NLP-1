{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# The Stanford POS Tagger\n"
      ],
      "metadata": {
        "id": "TOiBlh5oa6ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Web app version: http://nlp.stanford.edu:8080/parser/ ; https://corenlp.run/\n",
        "\n",
        "Newer version of the NLTK interface, requires running a java server locally: https://github.com/nltk/nltk/wiki/Stanford-CoreNLP-API-in-NLTK\n"
      ],
      "metadata": {
        "id": "cfHeXLEUgIKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the tagger and models"
      ],
      "metadata": {
        "id": "MOXWLw-SbCpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and uzip the model. You can do the same thing on your own computer to be able to use it locally."
      ],
      "metadata": {
        "id": "rmImjTN1bZMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "!wget 'https://nlp.stanford.edu/software/stanford-tagger-4.2.0.zip'\n",
        "!unzip './stanford-tagger-4.2.0.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nYqHkCnbF0o",
        "outputId": "c878f3df-2a2c-4d50-d6cd-6d2e122dc365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-25 19:59:21--  https://nlp.stanford.edu/software/stanford-tagger-4.2.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-tagger-4.2.0.zip [following]\n",
            "--2023-03-25 19:59:21--  https://downloads.cs.stanford.edu/nlp/software/stanford-tagger-4.2.0.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 78034596 (74M) [application/zip]\n",
            "Saving to: ‘stanford-tagger-4.2.0.zip’\n",
            "\n",
            "stanford-tagger-4.2 100%[===================>]  74.42M  5.11MB/s    in 11s     \n",
            "\n",
            "2023-03-25 19:59:32 (6.96 MB/s) - ‘stanford-tagger-4.2.0.zip’ saved [78034596/78034596]\n",
            "\n",
            "Archive:  ./stanford-tagger-4.2.0.zip\n",
            "   creating: stanford-postagger-full-2020-11-17/\n",
            "  inflating: stanford-postagger-full-2020-11-17/stanford-postagger-gui.sh  \n",
            "  inflating: stanford-postagger-full-2020-11-17/sample-input.txt  \n",
            "  inflating: stanford-postagger-full-2020-11-17/stanford-postagger.jar  \n",
            "   creating: stanford-postagger-full-2020-11-17/data/\n",
            "  inflating: stanford-postagger-full-2020-11-17/data/enclitic-inflections.data  \n",
            "  inflating: stanford-postagger-full-2020-11-17/stanford-postagger-4.2.0-javadoc.jar  \n",
            "  inflating: stanford-postagger-full-2020-11-17/README.txt  \n",
            "  inflating: stanford-postagger-full-2020-11-17/stanford-postagger-4.2.0.jar  \n",
            "  inflating: stanford-postagger-full-2020-11-17/stanford-postagger.sh  \n",
            "   creating: stanford-postagger-full-2020-11-17/models/\n",
            "  inflating: stanford-postagger-full-2020-11-17/models/english-left3words-distsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/spanish-ud.tagger.props  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/english-caseless-left3words-distsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/spanish-ud.tagger  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/arabic-train.tagger  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/english-left3words-distsim.tagger  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/english-bidirectional-distsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/arabic.tagger  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/chinese-distsim.tagger  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/german-ud.tagger.props  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/french-ud.tagger.props  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/arabic.tagger.props  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/english-caseless-left3words-distsim.tagger  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/french-ud.tagger  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/english-bidirectional-distsim.tagger  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/chinese-nodistsim.tagger  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/chinese-distsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/german-ud.tagger  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/README-Models.txt  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/chinese-nodistsim.tagger.props  \n",
            "  inflating: stanford-postagger-full-2020-11-17/models/arabic-train.tagger.props  \n",
            "  inflating: stanford-postagger-full-2020-11-17/stanford-postagger-gui.bat  \n",
            "  inflating: stanford-postagger-full-2020-11-17/build.xml  \n",
            "  inflating: stanford-postagger-full-2020-11-17/TaggerDemo2.java  \n",
            "  inflating: stanford-postagger-full-2020-11-17/sample-output.txt  \n",
            "  inflating: stanford-postagger-full-2020-11-17/stanford-postagger.bat  \n",
            "  inflating: stanford-postagger-full-2020-11-17/TaggerDemo.java  \n",
            "  inflating: stanford-postagger-full-2020-11-17/stanford-postagger-4.2.0-sources.jar  \n",
            "  inflating: stanford-postagger-full-2020-11-17/LICENSE.txt  \n",
            "CPU times: user 227 ms, sys: 37 ms, total: 264 ms\n",
            "Wall time: 16.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up and using the tagger with NLTK"
      ],
      "metadata": {
        "id": "gT_y9OzBbGwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path='./stanford-postagger-full-2020-11-17/models/english-bidirectional-distsim.tagger'\n",
        "jar_tagger_path='./stanford-postagger-full-2020-11-17/stanford-postagger-4.2.0.jar'"
      ],
      "metadata": {
        "id": "xUr6gPwJFKuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e8EcS6qqVL5"
      },
      "outputs": [],
      "source": [
        "from nltk.tag.stanford import StanfordPOSTagger # -- deprecated?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep nltk # needs nltk >= 3.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kRn3vy0SYlB",
        "outputId": "f2c415af-4283-4e66-d55d-9a5dd1cfd615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nltk==3.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger=StanfordPOSTagger(model_path, jar_tagger_path)\n"
      ],
      "metadata": {
        "id": "t6dDj5bcErgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNGYvc7GWfrX",
        "outputId": "53aacc9c-b907-4623-d195-b2fafb15201a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag(nltk.word_tokenize(\"I am eating a lot of candy.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovqmsl_yE1ac",
        "outputId": "0db57fb5-b6b4-4805-aa41-40fdb4615ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('eating', 'VBG'),\n",
              " ('a', 'DT'),\n",
              " ('lot', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('candy', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag(nltk.word_tokenize(\"Time flies like an arrow.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoauGuMcmT0C",
        "outputId": "1ffe346f-2793-4816-9758-f9600c58cb61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Time', 'NNP'),\n",
              " ('flies', 'VBZ'),\n",
              " ('like', 'IN'),\n",
              " ('an', 'DT'),\n",
              " ('arrow', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag(nltk.word_tokenize(\"Fruit flies like a banana.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9VN5EdHWQzQ",
        "outputId": "f640a913-741c-4f46-d042-dd683385310f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Fruit', 'NNP'),\n",
              " ('flies', 'VBZ'),\n",
              " ('like', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('banana', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag(nltk.word_tokenize(\"I don't like fruit flies like a banana.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD98Iz1EmUE2",
        "outputId": "9894ce9b-0521-4698-b3c8-ad7a25577fc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('do', 'VBP'),\n",
              " (\"n't\", 'RB'),\n",
              " ('like', 'VB'),\n",
              " ('fruit', 'NN'),\n",
              " ('flies', 'NNS'),\n",
              " ('like', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('banana', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag(nltk.word_tokenize(\"I am eating a lot of candy.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4sjjsmG4Jc7",
        "outputId": "a2c6af0e-2596-4499-dfc8-fe171af23bc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('eating', 'VBG'),\n",
              " ('a', 'DT'),\n",
              " ('lot', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('candy', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "st = PorterStemmer()\n",
        "tagger.tag([st.stem(t)\n",
        "      for t in nltk.word_tokenize(\"I am eating a lot of candy.\")])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjcJgsNAFDl6",
        "outputId": "c79b5d4e-12a9-4e90-a9ae-b9ccafb2c053"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('eat', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('lot', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('candi', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using averaged_perceptron_tagger in NLTK"
      ],
      "metadata": {
        "id": "eOhBNvOHjbgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oiS8XGnajjP",
        "outputId": "1b5af4f6-58b9-4950-a6db-fdc355273b12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(nltk.word_tokenize(\"I am eating a lot of candy.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trUEM4Y3jj1t",
        "outputId": "4d49753a-862b-4edc-a404-e56b8647c93d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('eating', 'VBG'),\n",
              " ('a', 'DT'),\n",
              " ('lot', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('candy', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(nltk.word_tokenize(\"Time flies like an arrow.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CmrSdotFjrpk",
        "outputId": "d9be56b4-904f-4826-ecde-67b02a7a57cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Time', 'NNP'),\n",
              " ('flies', 'NNS'),\n",
              " ('like', 'IN'),\n",
              " ('an', 'DT'),\n",
              " ('arrow', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(nltk.word_tokenize(\"Fruit flies like a banana.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "213h9xIbmMcF",
        "outputId": "43c04326-758b-4053-be53-a0a7e4520403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Fruit', 'NNP'),\n",
              " ('flies', 'VBZ'),\n",
              " ('like', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('banana', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the tags"
      ],
      "metadata": {
        "id": "UPjryCPmcUhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('tagsets')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cvzHwm3cWOg",
        "outputId": "6ec3dfa9-3e2c-43e7-bc34-25b5b07f2cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Unzipping help/tagsets.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.help.upenn_tagset('NNP')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmE9xp0ocdT4",
        "outputId": "1893ffd9-09df-40c0-a503-d07adb9ffd83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NNP: noun, proper, singular\n",
            "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
            "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
            "    Shannon A.K.C. Meltex Liverpool ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e1aZEtcncfM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tagged corpora"
      ],
      "metadata": {
        "id": "BdLxIkm7-nER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('brown')\n",
        "nltk.corpus.brown.tagged_words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyMcw22io47Y",
        "outputId": "ace62b84-f672-4c0c-cccd-1ec6104bd8b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(nltk.corpus.brown.tagged_sents(categories='news'))[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDispBNWHx9A",
        "outputId": "bf2e3f7f-377a-40f6-e406-6d4ae422f1fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[('The', 'AT'),\n",
              "  ('Fulton', 'NP-TL'),\n",
              "  ('County', 'NN-TL'),\n",
              "  ('Grand', 'JJ-TL'),\n",
              "  ('Jury', 'NN-TL'),\n",
              "  ('said', 'VBD'),\n",
              "  ('Friday', 'NR'),\n",
              "  ('an', 'AT'),\n",
              "  ('investigation', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  (\"Atlanta's\", 'NP$'),\n",
              "  ('recent', 'JJ'),\n",
              "  ('primary', 'NN'),\n",
              "  ('election', 'NN'),\n",
              "  ('produced', 'VBD'),\n",
              "  ('``', '``'),\n",
              "  ('no', 'AT'),\n",
              "  ('evidence', 'NN'),\n",
              "  (\"''\", \"''\"),\n",
              "  ('that', 'CS'),\n",
              "  ('any', 'DTI'),\n",
              "  ('irregularities', 'NNS'),\n",
              "  ('took', 'VBD'),\n",
              "  ('place', 'NN'),\n",
              "  ('.', '.')],\n",
              " [('The', 'AT'),\n",
              "  ('jury', 'NN'),\n",
              "  ('further', 'RBR'),\n",
              "  ('said', 'VBD'),\n",
              "  ('in', 'IN'),\n",
              "  ('term-end', 'NN'),\n",
              "  ('presentments', 'NNS'),\n",
              "  ('that', 'CS'),\n",
              "  ('the', 'AT'),\n",
              "  ('City', 'NN-TL'),\n",
              "  ('Executive', 'JJ-TL'),\n",
              "  ('Committee', 'NN-TL'),\n",
              "  (',', ','),\n",
              "  ('which', 'WDT'),\n",
              "  ('had', 'HVD'),\n",
              "  ('over-all', 'JJ'),\n",
              "  ('charge', 'NN'),\n",
              "  ('of', 'IN'),\n",
              "  ('the', 'AT'),\n",
              "  ('election', 'NN'),\n",
              "  (',', ','),\n",
              "  ('``', '``'),\n",
              "  ('deserves', 'VBZ'),\n",
              "  ('the', 'AT'),\n",
              "  ('praise', 'NN'),\n",
              "  ('and', 'CC'),\n",
              "  ('thanks', 'NNS'),\n",
              "  ('of', 'IN'),\n",
              "  ('the', 'AT'),\n",
              "  ('City', 'NN-TL'),\n",
              "  ('of', 'IN-TL'),\n",
              "  ('Atlanta', 'NP-TL'),\n",
              "  (\"''\", \"''\"),\n",
              "  ('for', 'IN'),\n",
              "  ('the', 'AT'),\n",
              "  ('manner', 'NN'),\n",
              "  ('in', 'IN'),\n",
              "  ('which', 'WDT'),\n",
              "  ('the', 'AT'),\n",
              "  ('election', 'NN'),\n",
              "  ('was', 'BEDZ'),\n",
              "  ('conducted', 'VBN'),\n",
              "  ('.', '.')]]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Penn Treebank Corpus"
      ],
      "metadata": {
        "id": "f87x_HaCAc5o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "Paper with Penn Treebank description: https://www.researchgate.net/publication/2873803_The_Penn_Treebank_An_overview\n",
        "\n",
        "List of explanations for tag acronyms: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n"
      ],
      "metadata": {
        "id": "NUHBOwuFI1-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('treebank')\n",
        "len(list(nltk.corpus.treebank.tagged_words()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzKpPn3b-t59",
        "outputId": "2c2ef9b6-f868-411e-dda3-7d0f1303ea35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100676"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.corpus.treebank.tagged_words()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jj6m7U-Ag5v",
        "outputId": "d7856ea2-a3bf-4b6d-e3f3-3427f8f57876"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ...]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other tools, POS Tagging performance & comparison: https://aclweb.org/aclwiki/index.php?title=POS_Tagging_(State_of_the_art)"
      ],
      "metadata": {
        "id": "FFZ0yM74MT8e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Syntactic Parsing"
      ],
      "metadata": {
        "id": "BVvUf_bfjv7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stanford Parser"
      ],
      "metadata": {
        "id": "USY05F9NHDfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try the usage below locally: (needs java)"
      ],
      "metadata": {
        "id": "1wW5TEazbjwV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://nlp.stanford.edu/software/stanford-parser-4.2.0.zip'\n",
        "!unzip 'stanford-parser-4.2.0.zip'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWCNPBqTnEej",
        "outputId": "05001ec7-3f9c-417b-8f49-54b9a16f4696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-25 20:00:03--  https://nlp.stanford.edu/software/stanford-parser-4.2.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-parser-4.2.0.zip [following]\n",
            "--2023-03-25 20:00:03--  https://downloads.cs.stanford.edu/nlp/software/stanford-parser-4.2.0.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 182285548 (174M) [application/zip]\n",
            "Saving to: ‘stanford-parser-4.2.0.zip’\n",
            "\n",
            "stanford-parser-4.2 100%[===================>] 173.84M  5.08MB/s    in 30s     \n",
            "\n",
            "2023-03-25 20:00:34 (5.71 MB/s) - ‘stanford-parser-4.2.0.zip’ saved [182285548/182285548]\n",
            "\n",
            "Archive:  stanford-parser-4.2.0.zip\n",
            "   creating: stanford-parser-full-2020-11-17/\n",
            "  inflating: stanford-parser-full-2020-11-17/ejml-simple-0.38.jar  \n",
            "  inflating: stanford-parser-full-2020-11-17/StanfordDependenciesManual.pdf  \n",
            "  inflating: stanford-parser-full-2020-11-17/Makefile  \n",
            "  inflating: stanford-parser-full-2020-11-17/ShiftReduceDemo.java  \n",
            "  inflating: stanford-parser-full-2020-11-17/slf4j-api-1.7.12-sources.jar  \n",
            "  inflating: stanford-parser-full-2020-11-17/ejml-core-0.38.jar  \n",
            "  inflating: stanford-parser-full-2020-11-17/build.xml  \n",
            "  inflating: stanford-parser-full-2020-11-17/lexparser-gui.sh  \n",
            "   creating: stanford-parser-full-2020-11-17/data/\n",
            " extracting: stanford-parser-full-2020-11-17/data/chinese-onesent-unseg-gb18030.txt  \n",
            " extracting: stanford-parser-full-2020-11-17/data/arabic-onesent-utf8.txt  \n",
            "  inflating: stanford-parser-full-2020-11-17/data/chinese-onesent-unseg-utf8.txt  \n",
            "  inflating: stanford-parser-full-2020-11-17/data/testsent.txt  \n",
            "  inflating: stanford-parser-full-2020-11-17/data/pos-sentences.txt  \n",
            " extracting: stanford-parser-full-2020-11-17/data/chinese-onesent-utf8.txt  \n",
            " extracting: stanford-parser-full-2020-11-17/data/chinese-onesent-gb18030.txt  \n",
            " extracting: stanford-parser-full-2020-11-17/data/english-onesent.txt  \n",
            " extracting: stanford-parser-full-2020-11-17/data/german-onesent.txt  \n",
            "  inflating: stanford-parser-full-2020-11-17/data/french-onesent.txt  \n",
            "  inflating: stanford-parser-full-2020-11-17/LICENSE.txt  \n",
            "  inflating: stanford-parser-full-2020-11-17/stanford-parser-4.2.0-sources.jar  \n",
            "  inflating: stanford-parser-full-2020-11-17/lexparser-gui.command  \n",
            "  inflating: stanford-parser-full-2020-11-17/README_dependencies.txt  \n",
            "  inflating: stanford-parser-full-2020-11-17/ejml-ddense-0.38.jar  \n",
            "   creating: stanford-parser-full-2020-11-17/conf/\n",
            "  inflating: stanford-parser-full-2020-11-17/conf/ftb-latest.conf  \n",
            "  inflating: stanford-parser-full-2020-11-17/conf/atb-latest.conf  \n",
            "  inflating: stanford-parser-full-2020-11-17/ejml-ddense-0.39-sources.jar  \n",
            "  inflating: stanford-parser-full-2020-11-17/ejml-simple-0.39-sources.jar  \n",
            "  inflating: stanford-parser-full-2020-11-17/README.txt  \n",
            "  inflating: stanford-parser-full-2020-11-17/slf4j-simple.jar  \n",
            "  inflating: stanford-parser-full-2020-11-17/lexparser-lang-train-test.sh  \n",
            "  inflating: stanford-parser-full-2020-11-17/stanford-parser-4.2.0-models.jar  \n",
            "  inflating: stanford-parser-full-2020-11-17/stanford-parser-4.2.0-javadoc.jar  \n",
            "  inflating: stanford-parser-full-2020-11-17/lexparser.sh  \n",
            "  inflating: stanford-parser-full-2020-11-17/lexparser-lang.sh  \n",
            "  inflating: stanford-parser-full-2020-11-17/lexparser.bat  \n",
            "  inflating: stanford-parser-full-2020-11-17/ParserDemo2.java  \n",
            "  inflating: stanford-parser-full-2020-11-17/pom.xml  \n",
            "  inflating: stanford-parser-full-2020-11-17/ejml-core-0.39-sources.jar  \n",
            "  inflating: stanford-parser-full-2020-11-17/stanford-parser.jar  \n",
            "  inflating: stanford-parser-full-2020-11-17/DependencyParserDemo.java  \n",
            "   creating: stanford-parser-full-2020-11-17/bin/\n",
            "  inflating: stanford-parser-full-2020-11-17/bin/makeSerialized.csh  \n",
            "  inflating: stanford-parser-full-2020-11-17/bin/run-tb-preproc  \n",
            "  inflating: stanford-parser-full-2020-11-17/slf4j-api.jar  \n",
            "  inflating: stanford-parser-full-2020-11-17/ParserDemo.java  \n",
            "  inflating: stanford-parser-full-2020-11-17/lexparser_lang.def  \n",
            "  inflating: stanford-parser-full-2020-11-17/lexparser-gui.bat  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget 'https://nlp.stanford.edu/software/stanford-corenlp-4.2.0-models-english.jar'"
      ],
      "metadata": {
        "id": "_YSxVuhnoFCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nltk.parse.stanford import StanfordParser\n"
      ],
      "metadata": {
        "id": "mRHcRHDgj0Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['JAVAHOME'] = \"/usr/bin/java\"\n",
        "os.environ['STANFORD_PARSER'] = '/content/stanford-parser-full-2020-11-17/stanford-parser.jar'\n",
        "os.environ['STANFORD_MODELS'] = '/content/stanford-parser-full-2020-11-17/stanford-parser-4.2.0-models.jar'\n"
      ],
      "metadata": {
        "id": "vi-uBl6yk3HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "parser = StanfordParser(model_path=\"/content/stanford-corenlp-4.2.0-models-english.jar\")\n",
        "propozitii = parser.raw_parse_sents((\"I like to go to school.\", \"The cat is running through the room.\",\"Where are you?\"))\n",
        "for prop in propozitii:\n",
        "    print(list(prop))"
      ],
      "metadata": {
        "id": "k-4RL7YelPyW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "outputId": "5318210f-ea41-4b2c-9e8b-a974208f7ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-dd9aad16e19a>:1: DeprecationWarning: The StanfordParser will be deprecated\n",
            "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
            "  parser = StanfordParser(model_path=\"/content/stanford-corenlp-4.2.0-models-english.jar\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main] ERROR edu.stanford.nlp.parser.lexparser.LexicalizedParser - java.io.IOException: Unable to open \"/content/stanford-corenlp-4.2.0-models-english.jar\" as class path, filename or URL\n",
            "  edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:501)\n",
            "  edu.stanford.nlp.io.IOUtils.readStreamFromString(IOUtils.java:402)\n",
            "  edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromSerializedFile(LexicalizedParser.java:567)\n",
            "  edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:373)\n",
            "  edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:183)\n",
            "  edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(LexicalizedParser.java:1373)\n",
            "java.io.IOException: Unable to open \"/content/stanford-corenlp-4.2.0-models-english.jar\" as class path, filename or URL\n",
            "\tat edu.stanford.nlp.io.IOUtils.getInputStreamFromURLOrClasspathOrFileSystem(IOUtils.java:501)\n",
            "\tat edu.stanford.nlp.io.IOUtils.readerFromString(IOUtils.java:634)\n",
            "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromTextFile(LexicalizedParser.java:510)\n",
            "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParser.getParserFromFile(LexicalizedParser.java:375)\n",
            "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:183)\n",
            "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(LexicalizedParser.java:1373)\n",
            "Exception in thread \"main\" java.lang.NullPointerException\n",
            "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParser.loadModel(LexicalizedParser.java:185)\n",
            "\tat edu.stanford.nlp.parser.lexparser.LexicalizedParser.main(LexicalizedParser.java:1373)\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-dd9aad16e19a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/stanford-corenlp-4.2.0-models-english.jar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpropozitii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"I like to go to school.\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The cat is running through the room.\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Where are you?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mprop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpropozitii\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36mraw_parse_sents\u001b[0;34m(self, sentences, verbose)\u001b[0m\n\u001b[1;32m    171\u001b[0m         ]\n\u001b[1;32m    172\u001b[0m         return self._parse_trees_output(\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         )\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/parse/stanford.py\u001b[0m in \u001b[0;36m_execute\u001b[0;34m(self, cmd, input_, verbose)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0mcmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m                 stdout, stderr = java(\n\u001b[0m\u001b[1;32m    259\u001b[0m                     \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasspath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_classpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/nltk/internals.py\u001b[0m in \u001b[0;36mjava\u001b[0;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decode_stdoutdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java command failed : \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: Java command failed : ['/usr/bin/java', '-mx4g', '-cp', '/content/stanford-parser-full-2020-11-17/stanford-parser-4.2.0-models.jar:/content/stanford-parser-full-2020-11-17/slf4j-api-1.7.12-sources.jar:/content/stanford-parser-full-2020-11-17/ejml-simple-0.39-sources.jar:/content/stanford-parser-full-2020-11-17/ejml-core-0.38.jar:/content/stanford-parser-full-2020-11-17/ejml-ddense-0.39-sources.jar:/content/stanford-parser-full-2020-11-17/slf4j-simple.jar:/content/stanford-parser-full-2020-11-17/ejml-ddense-0.38.jar:/content/stanford-parser-full-2020-11-17/ejml-simple-0.38.jar:/content/stanford-parser-full-2020-11-17/stanford-parser-4.2.0-sources.jar:/content/stanford-parser-full-2020-11-17/slf4j-api.jar:/content/stanford-parser-full-2020-11-17/ejml-core-0.39-sources.jar:/content/stanford-parser-full-2020-11-17/stanford-parser-4.2.0-javadoc.jar:/content/stanford-parser-full-2020-11-17/stanford-parser-4.2.0-models.jar:/content/stanford-parser-full-2020-11-17/stanford-parser.jar', 'edu.stanford.nlp.parser.lexparser.LexicalizedParser', '-model', '/content/stanford-corenlp-4.2.0-models-english.jar', '-sentences', 'newline', '-outputFormat', 'penn', '-encoding', 'utf8', '/tmp/tmph3j0ucgx']"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternatively\n"
      ],
      "metadata": {
        "id": "j5nUR1Y3HHpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://nlp.stanford.edu/software/stanford-corenlp-4.5.3.zip'\n",
        "!unzip 'stanford-corenlp-4.5.3.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-dAKHb7QIpI",
        "outputId": "40475cf1-b5f7-4e03-fe4b-cfd7b01049bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-25 20:01:13--  https://nlp.stanford.edu/software/stanford-corenlp-4.5.3.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.5.3.zip [following]\n",
            "--2023-03-25 20:01:14--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-4.5.3.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 505406322 (482M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-4.5.3.zip’\n",
            "\n",
            "stanford-corenlp-4. 100%[===================>] 481.99M  5.13MB/s    in 91s     \n",
            "\n",
            "2023-03-25 20:02:45 (5.29 MB/s) - ‘stanford-corenlp-4.5.3.zip’ saved [505406322/505406322]\n",
            "\n",
            "Archive:  stanford-corenlp-4.5.3.zip\n",
            "   creating: stanford-corenlp-4.5.3/\n",
            "  inflating: stanford-corenlp-4.5.3/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-4.5.3/jollyday-0.4.9-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/javax.activation-api-1.2.0.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/javax.json.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/ejml-ddense-0.39.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/input.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/istack-commons-runtime-3.0.7.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/README.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/stanford-corenlp-4.5.3-models.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/ejml-core-0.39-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-4.5.3/xom.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/jollyday.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/pom-java-17.xml  \n",
            "  inflating: stanford-corenlp-4.5.3/ejml-simple-0.39.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/input.txt.out  \n",
            "  inflating: stanford-corenlp-4.5.3/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-4.5.3/build.xml  \n",
            "   creating: stanford-corenlp-4.5.3/tokensregex/\n",
            "  inflating: stanford-corenlp-4.5.3/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-4.5.3/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/sample-project-pom.xml  \n",
            "  inflating: stanford-corenlp-4.5.3/corenlp.sh  \n",
            "  inflating: stanford-corenlp-4.5.3/pom-java-11.xml  \n",
            "  inflating: stanford-corenlp-4.5.3/stanford-corenlp-4.5.3-javadoc.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/protobuf-java-3.19.6.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/RESOURCE-LICENSES  \n",
            "  inflating: stanford-corenlp-4.5.3/StanfordDependenciesManual.pdf  \n",
            "   creating: stanford-corenlp-4.5.3/patterns/\n",
            " extracting: stanford-corenlp-4.5.3/patterns/places.txt  \n",
            " extracting: stanford-corenlp-4.5.3/patterns/otherpeople.txt  \n",
            " extracting: stanford-corenlp-4.5.3/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/patterns/stopwords.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/patterns/names.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/patterns/example.properties  \n",
            "  inflating: stanford-corenlp-4.5.3/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/javax.activation-api-1.2.0-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/ejml-core-0.39.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/LIBRARY-LICENSES  \n",
            "  inflating: stanford-corenlp-4.5.3/stanford-corenlp-4.5.3.jar  \n",
            "   creating: stanford-corenlp-4.5.3/sutime/\n",
            "  inflating: stanford-corenlp-4.5.3/sutime/spanish.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/sutime/english.holidays.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/sutime/british.sutime.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/input.txt.xml  \n",
            "  inflating: stanford-corenlp-4.5.3/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/LICENSE.txt  \n",
            "  inflating: stanford-corenlp-4.5.3/jaxb-impl-2.4.0-b180830.0438-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/xom-1.3.8-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/Makefile  \n",
            "  inflating: stanford-corenlp-4.5.3/stanford-corenlp-4.5.3-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/jaxb-api-2.4.0-b180830.0359-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/jaxb-impl-2.4.0-b180830.0438.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/pom.xml  \n",
            "  inflating: stanford-corenlp-4.5.3/joda-time-2.10.5-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/ejml-simple-0.39-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/ejml-ddense-0.39-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/joda-time.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/istack-commons-runtime-3.0.7-sources.jar  \n",
            "  inflating: stanford-corenlp-4.5.3/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-4.5.3/jaxb-api-2.4.0-b180830.0359.jar  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanfordcorenlp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw_7gV3pHJpQ",
        "outputId": "da9a2c0b-db87-4d49-e54a-59bd397b7fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stanfordcorenlp\n",
            "  Downloading stanfordcorenlp-3.9.1.1-py2.py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from stanfordcorenlp) (2.27.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from stanfordcorenlp) (5.9.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->stanfordcorenlp) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->stanfordcorenlp) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->stanfordcorenlp) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->stanfordcorenlp) (3.4)\n",
            "Installing collected packages: stanfordcorenlp\n",
            "Successfully installed stanfordcorenlp-3.9.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependency parsing"
      ],
      "metadata": {
        "id": "njw19ohd-RLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import stanfordcorenlp\n",
        "sc = stanfordcorenlp.StanfordCoreNLP('stanford-corenlp-4.5.3')"
      ],
      "metadata": {
        "id": "BVPPE8TePAxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I eat a lot of candy.\"\n",
        "dependencies = sc.dependency_parse(text)\n",
        "dependencies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CAbPIuRQ2Hy",
        "outputId": "b934821a-0c70-4257-ed9d-add4b6bbad9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('ROOT', 0, 2),\n",
              " ('nsubj', 2, 1),\n",
              " ('det', 4, 3),\n",
              " ('obj', 2, 4),\n",
              " ('case', 6, 5),\n",
              " ('nmod', 4, 6),\n",
              " ('punct', 2, 7)]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = nltk.word_tokenize(text)\n",
        "for (t, w1, w2) in dependencies:\n",
        "  if w1 < len(tokens) and w2 < len(tokens):\n",
        "    print(\"%s --> %s (%s)\" % (\n",
        "        tokens[w2-1] if w2>0 else \"\",\n",
        "        tokens[w1-1] if w1>0 else \"\",\n",
        "         t))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpkrxm6dZCeh",
        "outputId": "484fdd98-16c5-479c-806e-fa32cd6c56c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat -->  (ROOT)\n",
            "I --> eat (nsubj)\n",
            "a --> lot (det)\n",
            "lot --> eat (obj)\n",
            "of --> candy (case)\n",
            "candy --> lot (nmod)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descriptions of dependency relations: https://universaldependencies.org/u/dep/\n",
        "\n",
        "Demo: https://nlp.stanford.edu/software/stanford-dependencies.html\n"
      ],
      "metadata": {
        "id": "a5u_0xRdfaP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constituent parsing"
      ],
      "metadata": {
        "id": "vwjQVOKv-NpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parsed = sc.parse(text)\n",
        "print(parsed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwAAqvOFQ8Ff",
        "outputId": "43495173-9722-4860-aa9b-5ef112dc86e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(ROOT\n",
            "  (S\n",
            "    (NP (PRP I))\n",
            "    (VP (VBP eat)\n",
            "      (NP\n",
            "        (NP (DT a) (NN lot))\n",
            "        (PP (IN of)\n",
            "          (NP (NN candy)))))\n",
            "    (. .)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing with custom grammar"
      ],
      "metadata": {
        "id": "0NMLbkBIqrU8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.nltk.org/book/ch08.html"
      ],
      "metadata": {
        "id": "UHEZ0ie6t9Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "gram = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP | TO VB | VB\n",
        "VP -> V NP | V NP PP | V S | V PP\n",
        "PP -> P NP\n",
        "V -> \"caught\" | \"ate\" | \"likes\" | \"like\" | \"chase\" | \"go\" | \"fly\" | \"flies\" | \"eat\" | \"saw\"\n",
        "NP -> Det N | Det N PP | PRP\n",
        "Det -> \"the\" | \"a\" | \"an\" | \"my\" | \"some\" | \"The\"\n",
        "N -> \"mice\" | \"cat\" | \"dog\" |  \"school\" | \"Time\" | \"arrow\" | \"fly\" | \"flies\" | \"candy\" | \"man\" | \"park\"\n",
        "P -> \"in\" | \"to\" | \"on\"\n",
        "TO -> \"to\"\n",
        "PRP -> \"I\"  \"\"\")\n"
      ],
      "metadata": {
        "id": "RISdpyTPqv-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdp = nltk.RecursiveDescentParser(gram)\n",
        "rdp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S--OvWijq10q",
        "outputId": "af179b40-4237-4344-be9d-4eeca07defc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nltk.parse.recursivedescent.RecursiveDescentParser at 0x7fb8fb8158b0>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I eat some candy\"\n",
        "for tree in rdp.parse(nltk.word_tokenize(text)):\n",
        "    print(tree)"
      ],
      "metadata": {
        "id": "Iu5HKnFhsyFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9d6faca-9b38-4be7-8b5f-5fa6351501e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP (PRP I)) (VP (V eat) (NP (Det some) (N candy))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Syntactic ambiguity:"
      ],
      "metadata": {
        "id": "ZKwCg4BgtCS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "gram = nltk.CFG.fromstring(\"\"\"\n",
        "S -> NP VP | TO VB\n",
        "PP -> P NP\n",
        "V -> \"caught\" | \"ate\" | \"likes\" | \"like\" | \"chase\" | \"go\" | \"fly\" | \"flies\" | \"eat\" | \"saw\"\n",
        "NP -> Det N | Det N PP | PRP\n",
        "Det -> \"the\" | \"a\" | \"an\" | \"my\" | \"some\" | \"The\"\n",
        "N -> \"mice\" | \"cat\" | \"dog\" |  \"school\" | \"Time\" | \"arrow\" | \"fly\" | \"flies\" | \"candy\" | \"man\" | \"park\"\n",
        "P -> \"in\" | \"to\" | \"on\"\n",
        "TO -> \"to\"\n",
        "PRP -> \"I\"  \"\"\")"
      ],
      "metadata": {
        "id": "DJPT-XHtlLJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdp = nltk.RecursiveDescentParser(gram, trace=2)\n",
        "rdp"
      ],
      "metadata": {
        "id": "7z9QRNU7vi4s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55de9840-b53b-4f33-9c27-0a91eac52068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nltk.parse.recursivedescent.RecursiveDescentParser at 0x7fb8fb44fc10>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The dog saw a man in the park\"\n",
        "for tree in rdp.parse(nltk.word_tokenize(text)):\n",
        "    print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I8fOSo-szj_",
        "outputId": "b0ac25f1-c58a-4d6f-b2c3-e464302df31e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing 'The dog saw a man in the park'\n",
            "    [ * S ]\n",
            "  E [ * NP VP ]\n",
            "  E [ * Det N VP ]\n",
            "  E [ * 'the' N VP ]\n",
            "  E [ * 'a' N VP ]\n",
            "  E [ * 'an' N VP ]\n",
            "  E [ * 'my' N VP ]\n",
            "  E [ * 'some' N VP ]\n",
            "  E [ * 'The' N VP ]\n",
            "  M [ 'The' * N VP ]\n",
            "  E [ 'The' * 'mice' VP ]\n",
            "  E [ 'The' * 'cat' VP ]\n",
            "  E [ 'The' * 'dog' VP ]\n",
            "  M [ 'The' 'dog' * VP ]\n",
            "  E [ 'The' * 'school' VP ]\n",
            "  E [ 'The' * 'Time' VP ]\n",
            "  E [ 'The' * 'arrow' VP ]\n",
            "  E [ 'The' * 'fly' VP ]\n",
            "  E [ 'The' * 'flies' VP ]\n",
            "  E [ 'The' * 'candy' VP ]\n",
            "  E [ 'The' * 'man' VP ]\n",
            "  E [ 'The' * 'park' VP ]\n",
            "  E [ * Det N PP VP ]\n",
            "  E [ * 'the' N PP VP ]\n",
            "  E [ * 'a' N PP VP ]\n",
            "  E [ * 'an' N PP VP ]\n",
            "  E [ * 'my' N PP VP ]\n",
            "  E [ * 'some' N PP VP ]\n",
            "  E [ * 'The' N PP VP ]\n",
            "  M [ 'The' * N PP VP ]\n",
            "  E [ 'The' * 'mice' PP VP ]\n",
            "  E [ 'The' * 'cat' PP VP ]\n",
            "  E [ 'The' * 'dog' PP VP ]\n",
            "  M [ 'The' 'dog' * PP VP ]\n",
            "  E [ 'The' 'dog' * P NP VP ]\n",
            "  E [ 'The' 'dog' * 'in' NP VP ]\n",
            "  E [ 'The' 'dog' * 'to' NP VP ]\n",
            "  E [ 'The' 'dog' * 'on' NP VP ]\n",
            "  E [ 'The' * 'school' PP VP ]\n",
            "  E [ 'The' * 'Time' PP VP ]\n",
            "  E [ 'The' * 'arrow' PP VP ]\n",
            "  E [ 'The' * 'fly' PP VP ]\n",
            "  E [ 'The' * 'flies' PP VP ]\n",
            "  E [ 'The' * 'candy' PP VP ]\n",
            "  E [ 'The' * 'man' PP VP ]\n",
            "  E [ 'The' * 'park' PP VP ]\n",
            "  E [ * PRP VP ]\n",
            "  E [ * 'I' VP ]\n",
            "  E [ * TO VB ]\n",
            "  E [ * 'to' VB ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kq1zcNBwKhib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "srp = nltk.ShiftReduceParser(gram, trace=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8Djg8VwvSkI",
        "outputId": "fe8e6f84-d072-4a82-ba13-ccf767f16f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: V -> 'fly' will never be used\n",
            "Warning: V -> 'flies' will never be used\n",
            "Warning: P -> 'to' will never be used\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"I like my candy\"\n",
        "for tree in srp.parse(nltk.word_tokenize(text)):\n",
        "    print(tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8YE9mlUvfxw",
        "outputId": "0dacd689-fe6f-44d6-8d0b-6b1288d308e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing 'I like my candy'\n",
            "    [ * I like my candy]\n",
            "  S [ 'I' * like my candy]\n",
            "  R [ PRP * like my candy]\n",
            "  R [ NP * like my candy]\n",
            "  S [ NP 'like' * my candy]\n",
            "  R [ NP V * my candy]\n",
            "  S [ NP V 'my' * candy]\n",
            "  R [ NP V Det * candy]\n",
            "  S [ NP V Det 'candy' * ]\n",
            "  R [ NP V Det N * ]\n",
            "  R [ NP V NP * ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.draw.tree import draw_trees\n",
        "\n",
        "# (works locally)\n",
        "for tree in srp.parse(nltk.word_tokenize(text)):\n",
        "    draw_trees(tree)"
      ],
      "metadata": {
        "id": "2HPGpj0sKnf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f9c5c3-5f68-4e48-aee6-1752d7bce40c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing 'I like my candy'\n",
            "    [ * I like my candy]\n",
            "  S [ 'I' * like my candy]\n",
            "  R [ PRP * like my candy]\n",
            "  R [ NP * like my candy]\n",
            "  S [ NP 'like' * my candy]\n",
            "  R [ NP V * my candy]\n",
            "  S [ NP V 'my' * candy]\n",
            "  R [ NP V Det * candy]\n",
            "  S [ NP V Det 'candy' * ]\n",
            "  R [ NP V Det N * ]\n",
            "  R [ NP V NP * ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
        " S -> NP VP\n",
        " PP -> P NP\n",
        " NP -> Det N | Det N PP | 'I'\n",
        " VP -> V NP | VP PP\n",
        " Det -> 'an' | 'my'\n",
        " N -> 'elephant' | 'pajamas'\n",
        " V -> 'shot'\n",
        " P -> 'in'\n",
        " \"\"\")"
      ],
      "metadata": {
        "id": "BFfd-eRQy5Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
        "parser = nltk.ChartParser(groucho_grammar)\n",
        "for tree in parser.parse(sent):\n",
        "  print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ggZ4VB4y-JL",
        "outputId": "c759f005-6ca4-4741-8ca2-338ea8934f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (VP (V shot) (NP (Det an) (N elephant)))\n",
            "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (V shot)\n",
            "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
        "parser = nltk.ShiftReduceParser(groucho_grammar)\n",
        "for tree in parser.parse(sent):\n",
        "  print(tree)"
      ],
      "metadata": {
        "id": "zeVp4k5volVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercitii (1p total)"
      ],
      "metadata": {
        "id": "H_1O3gu1A-Ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Completati functia scrisa in laboratorul interior cu POS-tagging: in final veti avea o functie care sa primeasca un text si sa faca toata preprocesarea (tokenizare, lematizare, normalizare) si pos-tagging, si sa intoarca textul cu tags pe cuvinte.\n",
        "\n",
        "/\n",
        "\n",
        "Add to the function implemented for the past lab instructions for POS-tagging: in the end you should have a function which receives an input text and performs preprocessing from beginning to end (tokenization, lemmatization, normalization) as well as POS-tagging, then returns the tagged text."
      ],
      "metadata": {
        "id": "wB4FM4LABFAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer, SnowballStemmer\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19mHsrWaxTDH",
        "outputId": "046cefe8-5dcd-4dcb-e046-675996a32887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blacklist_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "mHZ-cxGQxGFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagger=StanfordPOSTagger(model_path, jar_tagger_path)"
      ],
      "metadata": {
        "id": "BVIOnyXZy1aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocesare(text, lowercase=True, remove_numbers=False, remove_stopwords=True, stemming=None, lemmatization=None, punctuation=True, pos_tagger=True):\n",
        "\n",
        "    #remove text within paranthesis\n",
        "    text = re.sub(r'[\\(\\[\\{].*?[\\)\\]\\}]', '', text)\n",
        "\n",
        "    #tokenize words\n",
        "    text = nltk.word_tokenize(text, language='english')\n",
        "\n",
        "    #lowercase\n",
        "    if lowercase:\n",
        "        text = [t.lower() for t in text]\n",
        "\n",
        "    #remove punctuation\n",
        "    if punctuation:\n",
        "        text = [t for t in text if t not in string.punctuation]\n",
        "\n",
        "\n",
        "    #remove stopwords\n",
        "    if remove_stopwords:\n",
        "        text = [t for t in text if t not in blacklist_words]\n",
        "\n",
        "\n",
        "    if remove_numbers:\n",
        "        text = [t for t in text if re.match('^[a-z]+$', t)]\n",
        "\n",
        "    #choose stemmer\n",
        "    if stemming:\n",
        "        if stemming == 'porter':\n",
        "            stemmer = PorterStemmer()\n",
        "        elif stemming == 'snowball':\n",
        "            stemmer = SnowballStemmer('english')\n",
        "        else:\n",
        "            raise ValueError(\"Choose valid stemmer!\")\n",
        "        text = [stemmer.stem(t) for t in text]\n",
        "\n",
        "    #choose lemmatizer\n",
        "    if lemmatization:\n",
        "        if lemmatization == 'wordnet':\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "        else:\n",
        "            raise ValueError(\"Choose valid lemmatizer!\")\n",
        "        text = [lemmatizer.lemmatize(t) for t in text]\n",
        "\n",
        "    if pos_tagger:\n",
        "      text =  tagger.tag(text)\n",
        "\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "G27nCM7ZBVlS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Aplicati functia pe un fragment de cateva sute de cuvinte din stiri din ultimele cateva zile (aveti grija sa fie intr-o limba pe care functia o suporta). Afisati distributia partilor de vorbire intr-un grafic.\n",
        "\n",
        "/\n",
        "\n",
        "Execute your function on a piece of news from the past days of at least a few hundred words (make sure it's in a language that the function supports). Illustrate the distribution of POSs in a graph."
      ],
      "metadata": {
        "id": "jb70NTXABV2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = \"SEOUL, South Korea : A young lonely zebra had a rare day out when he ran away from a zoo in Seoul and trotted around the streets of the South Korean capital, before being sedated and captured a few hours later. Sero, a 3-year-old male whose Korean name refers to his vertical stripes, escaped from the Seoul Children’s Grand Park zoo Thursday afternoon by breaking through the wooden deck around his enclosure, according to zoo officials. In the hours that followed, Sero ran amok in a nearby residential area in eastern Seoul as people looked on in shock. Social media was instantly flooded with photos and videos of the zebra bumping into traffic and galloping through narrow alleyways.\""
      ],
      "metadata": {
        "id": "G5skYL9zCXaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text = preprocesare(news, lemmatization= 'wordnet')\n",
        "print(preprocessed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eauqdVJ7ZnQD",
        "outputId": "191d1de2-2ba7-4e4c-8183-5f77c0a25c02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('seoul', 'NNP'), ('south', 'NNP'), ('korea', 'NNP'), ('young', 'JJ'), ('lonely', 'JJ'), ('zebra', 'NN'), ('rare', 'JJ'), ('day', 'NN'), ('ran', 'VBD'), ('away', 'RB'), ('zoo', 'NN'), ('seoul', 'NN'), ('trotted', 'VBD'), ('around', 'IN'), ('street', 'NN'), ('south', 'RB'), ('korean', 'JJ'), ('capital', 'NN'), ('sedated', 'VBD'), ('captured', 'VBN'), ('hour', 'NN'), ('later', 'RB'), ('sero', 'VBD'), ('3-year-old', 'RB'), ('male', 'JJ'), ('whose', 'WP$'), ('korean', 'JJ'), ('name', 'NN'), ('refers', 'VBZ'), ('vertical', 'JJ'), ('stripe', 'NN'), ('escaped', 'VBD'), ('seoul', 'JJ'), ('child', 'NN'), ('’', \"''\"), ('grand', 'JJ'), ('park', 'NN'), ('zoo', 'NN'), ('thursday', 'NNP'), ('afternoon', 'NN'), ('breaking', 'VBG'), ('wooden', 'JJ'), ('deck', 'NN'), ('around', 'IN'), ('enclosure', 'NN'), ('according', 'VBG'), ('zoo', 'NN'), ('official', 'NN'), ('hour', 'NN'), ('followed', 'VBD'), ('sero', 'NN'), ('ran', 'VBD'), ('amok', 'RB'), ('nearby', 'RB'), ('residential', 'JJ'), ('area', 'NN'), ('eastern', 'JJ'), ('seoul', 'JJ'), ('people', 'NNS'), ('looked', 'VBD'), ('shock', 'NN'), ('social', 'JJ'), ('medium', 'NN'), ('instantly', 'RB'), ('flooded', 'VBN'), ('photo', 'NN'), ('video', 'NN'), ('zebra', 'NN'), ('bumping', 'VBG'), ('traffic', 'NN'), ('galloping', 'VBG'), ('narrow', 'JJ'), ('alleyway', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Modificati functia de mai sus astfel incat sa efectueze POS-tagging inainte sau dupa lematizare. Comparati diferentele. Afisati cuvintele (top 20) pentru care POS tag-ul identificat difera cel mai des intre cele doua versiuni.\n",
        "\n",
        "/\n",
        "\n",
        "Modify your function such that POS-tagging is performed before or after lemmatization. Print the words in the vocabulary (top 20) for which the identified POS tag differs most often between the two versions."
      ],
      "metadata": {
        "id": "sxDdel9eCYan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocesare(text, lowercase=True, remove_numbers=False, remove_stopwords=True, stemming=None, punctuation=True, pos_tagger=True, pos_tag_method=None):\n",
        "\n",
        "    #remove text within paranthesis\n",
        "    text = re.sub(r'[\\(\\[\\{].*?[\\)\\]\\}]', '', text)\n",
        "\n",
        "    #tokenize words\n",
        "    text = nltk.word_tokenize(text, language='english')\n",
        "\n",
        "    #lowercase\n",
        "    if lowercase:\n",
        "        text = [t.lower() for t in text]\n",
        "\n",
        "    #remove punctuation\n",
        "    if punctuation:\n",
        "        text = [t for t in text if t not in string.punctuation]\n",
        "\n",
        "\n",
        "    #remove stopwords\n",
        "    if remove_stopwords:\n",
        "        text = [t for t in text if t not in blacklist_words]\n",
        "\n",
        "\n",
        "    if remove_numbers:\n",
        "        text = [t for t in text if re.match('^[a-z]+$', t)]\n",
        "\n",
        "    if pos_tag_method == \"before\":\n",
        "      tagged_tokens =  tagger.tag(text)\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      tagged_text = []\n",
        "      for token, pos_tag in tagged_tokens:\n",
        "        lemma = lemmatizer.lemmatize(token)\n",
        "        tagged_text.append((lemma, pos_tag))\n",
        "      return tagged_text\n",
        "\n",
        "\n",
        "    elif pos_tag_method == \"after\":\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      text = [lemmatizer.lemmatize(t) for t in text]\n",
        "      text =  tagger.tag(text)\n",
        "      return text\n",
        "\n",
        "    else:\n",
        "      print(\"Choose a valid pos tag method!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "naOAjOjFb2Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text_before = preprocesare(news,  pos_tag_method=\"before\")\n",
        "preprocessed_text_after = preprocesare(news,  pos_tag_method=\"after\")\n",
        "print(preprocessed_text_before)\n",
        "print(preprocessed_text_after)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C8MucBfh2gT",
        "outputId": "408343a2-6e40-45bc-d978-ff13af327f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('seoul', 'NNP'), ('south', 'NNP'), ('korea', 'NNP'), ('young', 'JJ'), ('lonely', 'JJ'), ('zebra', 'NN'), ('rare', 'JJ'), ('day', 'NN'), ('ran', 'VBD'), ('away', 'RB'), ('zoo', 'NN'), ('seoul', 'NN'), ('trotted', 'VBD'), ('around', 'IN'), ('street', 'NNS'), ('south', 'RB'), ('korean', 'JJ'), ('capital', 'NN'), ('sedated', 'VBD'), ('captured', 'VBN'), ('hour', 'NNS'), ('later', 'RB'), ('sero', 'VBD'), ('3-year-old', 'RB'), ('male', 'JJ'), ('whose', 'WP$'), ('korean', 'JJ'), ('name', 'NN'), ('refers', 'VBZ'), ('vertical', 'JJ'), ('stripe', 'NNS'), ('escaped', 'VBD'), ('seoul', 'JJ'), ('child', 'NNS'), ('’', \"''\"), ('grand', 'JJ'), ('park', 'NN'), ('zoo', 'NN'), ('thursday', 'NNP'), ('afternoon', 'NN'), ('breaking', 'VBG'), ('wooden', 'JJ'), ('deck', 'NN'), ('around', 'IN'), ('enclosure', 'NN'), ('according', 'VBG'), ('zoo', 'NN'), ('official', 'NNS'), ('hour', 'NNS'), ('followed', 'VBD'), ('sero', 'NN'), ('ran', 'VBD'), ('amok', 'RB'), ('nearby', 'RB'), ('residential', 'JJ'), ('area', 'NN'), ('eastern', 'JJ'), ('seoul', 'JJ'), ('people', 'NNS'), ('looked', 'VBD'), ('shock', 'NN'), ('social', 'JJ'), ('medium', 'NNS'), ('instantly', 'RB'), ('flooded', 'VBN'), ('photo', 'NN'), ('video', 'NN'), ('zebra', 'NN'), ('bumping', 'VBG'), ('traffic', 'NN'), ('galloping', 'VBG'), ('narrow', 'JJ'), ('alleyway', 'NNS')]\n",
            "[('seoul', 'NNP'), ('south', 'NNP'), ('korea', 'NNP'), ('young', 'JJ'), ('lonely', 'JJ'), ('zebra', 'NN'), ('rare', 'JJ'), ('day', 'NN'), ('ran', 'VBD'), ('away', 'RB'), ('zoo', 'NN'), ('seoul', 'NN'), ('trotted', 'VBD'), ('around', 'IN'), ('street', 'NN'), ('south', 'RB'), ('korean', 'JJ'), ('capital', 'NN'), ('sedated', 'VBD'), ('captured', 'VBN'), ('hour', 'NN'), ('later', 'RB'), ('sero', 'VBD'), ('3-year-old', 'RB'), ('male', 'JJ'), ('whose', 'WP$'), ('korean', 'JJ'), ('name', 'NN'), ('refers', 'VBZ'), ('vertical', 'JJ'), ('stripe', 'NN'), ('escaped', 'VBD'), ('seoul', 'JJ'), ('child', 'NN'), ('’', \"''\"), ('grand', 'JJ'), ('park', 'NN'), ('zoo', 'NN'), ('thursday', 'NNP'), ('afternoon', 'NN'), ('breaking', 'VBG'), ('wooden', 'JJ'), ('deck', 'NN'), ('around', 'IN'), ('enclosure', 'NN'), ('according', 'VBG'), ('zoo', 'NN'), ('official', 'NN'), ('hour', 'NN'), ('followed', 'VBD'), ('sero', 'NN'), ('ran', 'VBD'), ('amok', 'RB'), ('nearby', 'RB'), ('residential', 'JJ'), ('area', 'NN'), ('eastern', 'JJ'), ('seoul', 'JJ'), ('people', 'NNS'), ('looked', 'VBD'), ('shock', 'NN'), ('social', 'JJ'), ('medium', 'NN'), ('instantly', 'RB'), ('flooded', 'VBN'), ('photo', 'NN'), ('video', 'NN'), ('zebra', 'NN'), ('bumping', 'VBG'), ('traffic', 'NN'), ('galloping', 'VBG'), ('narrow', 'JJ'), ('alleyway', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for before, after in zip(preprocessed_text_before, preprocessed_text_after):\n",
        "  if before != after:\n",
        "    print(before, after)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RA5MBZ9Kt1Fl",
        "outputId": "0cb2cfe6-5d59-43ad-b3d5-4f05f3089a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('street', 'NNS') ('street', 'NN')\n",
            "('hour', 'NNS') ('hour', 'NN')\n",
            "('stripe', 'NNS') ('stripe', 'NN')\n",
            "('child', 'NNS') ('child', 'NN')\n",
            "('official', 'NNS') ('official', 'NN')\n",
            "('hour', 'NNS') ('hour', 'NN')\n",
            "('medium', 'NNS') ('medium', 'NN')\n",
            "('alleyway', 'NNS') ('alleyway', 'NN')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Gasiti bigramele de POS tags cele mai frecvente in textul analizat, si apoi in propozitiile tagged din Brown corpus.\n",
        "\n",
        "/\n",
        "\n",
        "Find the most frequent POS tag bigrams occurring in the analyzed text, then in the tagged sentences in the Brown corpus."
      ],
      "metadata": {
        "id": "w7H6Dwdcb2qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist = nltk.FreqDist((word,tag) for word, tag in preprocessed_text_before)\n",
        "most_common = freq_dist.most_common(10)\n",
        "print( most_common)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUwSdLQ_3LMN",
        "outputId": "cc6771eb-1cf1-45c3-ec9b-daf5823c43fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('zoo', 'NN'), 3), (('zebra', 'NN'), 2), (('ran', 'VBD'), 2), (('around', 'IN'), 2), (('korean', 'JJ'), 2), (('hour', 'NNS'), 2), (('seoul', 'JJ'), 2), (('seoul', 'NNP'), 1), (('south', 'NNP'), 1), (('korea', 'NNP'), 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brown_tags = nltk.corpus.brown.tagged_words(tagset='brown')\n",
        "freq_dist = nltk.FreqDist((word.lower(), tag) for word, tag in brown_tags)\n",
        "most_common = freq_dist.most_common(10)\n",
        "print( most_common)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg8_mW0yyS0X",
        "outputId": "faaf8c01-cad9-4588-8c5d-72b70a5ea487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(('the', 'AT'), 69013), ((',', ','), 58153), (('.', '.'), 48812), (('of', 'IN'), 35028), (('and', 'CC'), 28542), (('a', 'AT'), 22943), (('in', 'IN'), 20731), (('to', 'TO'), 14917), (('to', 'IN'), 11046), (('is', 'BEZ'), 10065)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Write a grammar and code to produce two trees, one for each reading of the phrase \"old men and women\""
      ],
      "metadata": {
        "id": "cQ9ez4eHqcx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
        " S -> NP\n",
        " NP ->  ADJ NP | NP CC N\n",
        " NP -> N CC N | ADJ N\n",
        " N -> 'men' | 'women'\n",
        " ADJ -> 'old'\n",
        " CC -> 'and'\n",
        " \"\"\")"
      ],
      "metadata": {
        "id": "hupNTlUSBYqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = ['old', 'men', 'and', 'women']\n",
        "parser = nltk.ChartParser(groucho_grammar)\n",
        "for tree in parser.parse(sent):\n",
        "  print(tree)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAjLhKNoiFb_",
        "outputId": "db899158-747b-460c-c837-8ebf630728fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S (NP (NP (ADJ old) (N men)) (CC and) (N women)))\n",
            "(S (NP (ADJ old) (NP (N men) (CC and) (N women))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mini-proiect / Homework (+3p)"
      ],
      "metadata": {
        "id": "QNyFLIXOBBPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(v PDF separat)"
      ],
      "metadata": {
        "id": "2mdpLayrE-WC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WsqHgU4zAZdM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}