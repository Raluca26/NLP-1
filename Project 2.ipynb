{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cai_HFqlz0J4",
        "outputId": "94a39464-20dd-4c52-c29a-dc22df08a4e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from itertools import combinations\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Gutenberg corpus\n",
        "corpus = gutenberg.raw()\n",
        "\n",
        "def preprocess_text(text):\n",
        "  text = text.replace('\\n', ' ')\n",
        "  sentences = sent_tokenize(text)\n",
        "  tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]\n",
        "  tokenized_sentences = [[word.lower() for word in sentence] for sentence in tokenized_sentences]\n",
        "  tokenized_sentences = [[re.sub(r'[^a-zA-Z]', '', word) for word in sentence] for sentence in tokenized_sentences]\n",
        "  tokenized_sentences = [[word for word in sentence if word] for sentence in tokenized_sentences]\n",
        "\n",
        "  return tokenized_sentences\n",
        "\n",
        "preprocessed_corpus = preprocess_text(corpus)\n"
      ],
      "metadata": {
        "id": "I4vMRBfvz1Ei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of preprocessed_corpus: {len(preprocessed_corpus)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAqs4eFfz3sz",
        "outputId": "3114e393-500f-4e6f-bfa8-3e4c2245908d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of preprocessed_corpus: 94428\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_model = Word2Vec(\n",
        "    min_count=3,\n",
        "    window=8,\n",
        "    sg=0,  #  skip-gram (sg=1) or CBOW (sg=0)\n",
        "    seed=1\n",
        ")\n",
        "\n",
        "cbow_model.build_vocab(preprocessed_corpus)\n",
        "cbow_model.train(preprocessed_corpus, total_examples=cbow_model .corpus_count, epochs=1)\n",
        "voc = cbow_model.wv.index_to_key\n",
        "print(\"Tokens:\", len(voc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI36cWtFz5mt",
        "outputId": "c5a10ece-fbe5-4fa2-c316-91e87e805174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: 21192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "skipgram_model = Word2Vec(\n",
        "    min_count=3,\n",
        "    window=8,\n",
        "    sg=1,  #  skip-gram (sg=1) or CBOW (sg=0)\n",
        "    seed=1\n",
        ")\n",
        "\n",
        "skipgram_model.build_vocab(preprocessed_corpus)\n",
        "skipgram_model.train(preprocessed_corpus, total_examples=skipgram_model.corpus_count, epochs=1)\n",
        "voc = skipgram_model.wv.index_to_key\n",
        "print(\"Tokens:\", len(voc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JQlfDCUz7Z4",
        "outputId": "2dd88202-265e-4ceb-9d1f-a9fafc7334a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: 21192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PUNCTUL 1"
      ],
      "metadata": {
        "id": "DbERGMmp0DW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_wordnet_coverage(embedding_model):\n",
        "  total_words = len(embedding_model.wv.index_to_key)\n",
        "  wordnet_words = sum(\n",
        "    1 for word in embedding_model.wv.index_to_key\n",
        "    if wordnet.synsets(word) or\n",
        "      wordnet.synsets(word.lower()) or\n",
        "      wordnet.synsets(word.upper())\n",
        "    )\n",
        "\n",
        "  coverage = (wordnet_words / total_words) * 100\n",
        "  return coverage\n",
        "\n",
        "cbow_coverage = compute_wordnet_coverage(cbow_model)\n",
        "skipgram_coverage = compute_wordnet_coverage(skipgram_model)\n",
        "\n",
        "print(\"CBOW WordNet Coverage: {:.2f}%\".format(cbow_coverage))\n",
        "print(\"Skipgram WordNet Coverage: {:.2f}%\".format(skipgram_coverage))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ozYYYob0CxS",
        "outputId": "1987a9a2-d00d-4471-85fc-5569b353b25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBOW WordNet Coverage: 83.19%\n",
            "Skipgram WordNet Coverage: 83.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PUNCTUL 2"
      ],
      "metadata": {
        "id": "2VUwtw9x0O0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model):\n",
        "  non_stopword_words = [word for word in model.wv.index_to_key if word.lower() not in stopwords.words('english')][:1000]\n",
        "  thresholds = [0.6, 0.7, 0.8]\n",
        "\n",
        "  for threshold in thresholds:\n",
        "    synonyms_emb = 0\n",
        "    synonyms_wn = 0\n",
        "    common_synonyms = 0\n",
        "\n",
        "    for word_pair in combinations(non_stopword_words, 2):\n",
        "      similarity_emb = model.wv.similarity(word_pair[0], word_pair[1])\n",
        "      word1_lemma = WordNetLemmatizer().lemmatize(word_pair[0])\n",
        "      word2_lemma = WordNetLemmatizer().lemmatize(word_pair[1])\n",
        "      synsets_word1 = set(wn.synsets(word1_lemma))\n",
        "      synsets_word2 = set(wn.synsets(word2_lemma))\n",
        "      common_synsets = synsets_word1.intersection(synsets_word2)\n",
        "      similarity_wn = len(common_synsets)\n",
        "\n",
        "      if similarity_emb >= threshold:\n",
        "        synonyms_emb += 1\n",
        "\n",
        "      if similarity_wn > 0:\n",
        "        synonyms_wn += 1\n",
        "\n",
        "      if similarity_emb >= threshold and similarity_wn > 0:\n",
        "        common_synonyms += 1\n",
        "\n",
        "    precision = common_synonyms / synonyms_emb if synonyms_emb > 0 else 0.0\n",
        "    recall = common_synonyms / synonyms_wn if synonyms_wn > 0 else 0.0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
        "\n",
        "    print(f\"Threshold: {threshold} - Precision: {precision} - Recall: {recall} - F1-Score: {f1_score}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bBPDeFTYXeCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluate CBOW Model\")\n",
        "evaluate_model(cbow_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu8SBoJxFkPX",
        "outputId": "8667f42b-e4cb-47b2-f50e-e1af613fe4fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate CBOW Model\n",
            "Threshold: 0.6 - Precision: 0.0036946419849346436 - Recall: 0.9181514476614699 - F1-Score: 0.007359668658701503\n",
            "Threshold: 0.7 - Precision: 0.003778582736872307 - Recall: 0.8212694877505567 - F1-Score: 0.007522554914650878\n",
            "Threshold: 0.8 - Precision: 0.0038470985888205806 - Recall: 0.6191536748329621 - F1-Score: 0.007646684660214203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluate Skip_Gram Model\")\n",
        "evaluate_model(skipgram_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ubJ7lF7F_02",
        "outputId": "a0dea44d-e35f-4cd1-dd59-effd655daa2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate Skip_Gram Model\n",
            "Threshold: 0.6 - Precision: 0.003993884912854987 - Recall: 0.8858574610244989 - F1-Score: 0.007951918631530282\n",
            "Threshold: 0.7 - Precision: 0.00426456139141227 - Recall: 0.6887527839643652 - F1-Score: 0.008476637851580033\n",
            "Threshold: 0.8 - Precision: 0.004910874268346097 - Recall: 0.41202672605790647 - F1-Score: 0.009706063666531133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KsFZOEMOHecx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PUNCTUL 3\n"
      ],
      "metadata": {
        "id": "BbEso-CNK1Ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#a. Coverage Errors\n",
        "def coverage_errors(model):\n",
        "  emb_words = set(model.wv.index_to_key)\n",
        "  wordnet_words = set(wn.all_lemma_names())\n",
        "  coverage_errors = emb_words - wordnet_words\n",
        "  return list(coverage_errors)[:100]\n",
        "\n",
        "print(\"Coverage errors: CBOW Model\")\n",
        "print(coverage_errors(cbow_model))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXfDF2i4Ifb2",
        "outputId": "eff2e8db-c99e-4151-8ff8-6a99d581d26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coverage errors: CBOW Model\n",
            "['hor', 'screamed', 'custome', 'fulness', 'sufferings', 'falshood', 'bethaven', 'pathros', 'pharez', 'hailing', 'personall', 'realities', 'thirsted', 'bowlinggreen', 'fanatics', 'oughtest', 'tithes', 'receiveth', 'unskilful', 'undoubted', 'confederates', 'zerah', 'bethzur', 'shadows', 'seyward', 'stedfastly', 'obtained', 'uncircumcision', 'lep', 'drowne', 'seizes', 'spades', 'duncans', 'purchased', 'deceiveth', 'goings', 'endured', 'jehoahaz', 'testifying', 'jenning', 'drowning', 'opens', 'perches', 'purposing', 'embraced', 'remaineth', 'eluded', 'seacaptains', 'fords', 'draweth', 'courtiers', 'falleth', 'pelatiah', 'shoved', 'thieves', 'phantoms', 'forests', 'earning', 'wisest', 'intuitions', 'overcharged', 'owes', 'shells', 'craftsmen', 'cic', 'conceived', 'tophet', 'intending', 'shaul', 'whiles', 'this', 'delusions', 'groaned', 'ministering', 'knew', 'pulpits', 'skipping', 'reu', 'illustrates', 'aske', 'azaziah', 'excepting', 'fellowservants', 'flattered', 'prospereth', 'fatall', 'quinta', 'firme', 'speakes', 'beleeue', 'reared', 'flags', 'greatgrandfather', 've', 'recompensed', 'villains', 'galal', 'holpen', 'wizards', 'branches']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b. Precision Errors\n",
        "non_stopword_words = [word for word in cbow_model.wv.index_to_key if word.lower() not in stopwords.words('english')][:1000]\n",
        "threshold = 0.8\n",
        "precision_errors = []\n",
        "for word_pair in combinations(non_stopword_words, 2):\n",
        "  similarity_emb = cbow_model.wv.similarity(word_pair[0], word_pair[1])\n",
        "  word1_lemma = WordNetLemmatizer().lemmatize(word_pair[0])\n",
        "  word2_lemma = WordNetLemmatizer().lemmatize(word_pair[1])\n",
        "  synsets_word1 = set(wn.synsets(word1_lemma))\n",
        "  synsets_word2 = set(wn.synsets(word2_lemma))\n",
        "  common_synsets = synsets_word1.intersection(synsets_word2)\n",
        "  similarity_wn = len(common_synsets)\n",
        "  if similarity_emb >= threshold and similarity_wn == 0:\n",
        "    precision_errors.append(word_pair)\n",
        "print(\"Precision Errors:\", precision_errors[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLkwzgvhIfgt",
        "outputId": "9b17a825-0be0-4b56-cdf1-d1022a2b518b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision Errors: [('shall', 'let'), ('shall', 'may'), ('shall', 'must'), ('said', 'know'), ('said', 'well'), ('said', 'think'), ('said', 'therefore'), ('said', 'behold'), ('said', 'thought'), ('said', 'saith'), ('said', 'hear'), ('said', 'cried'), ('said', 'love'), ('said', 'oh'), ('said', 'sir'), ('said', 'always'), ('said', 'speak'), ('said', 'sure'), ('said', 'indeed'), ('said', 'enough'), ('said', 'answered'), ('said', 'dear'), ('said', 'keep'), ('said', 'fear'), ('said', 'hope'), ('said', 'yes'), ('said', 'believe'), ('said', 'call'), ('said', 'asked'), ('said', 'mean'), ('said', 'live'), ('said', 'alone'), ('said', 'matter'), ('said', 'wish'), ('said', 'answer'), ('said', 'replied'), ('said', 'ask'), ('said', 'help'), ('said', 'remember'), ('said', 'understand'), ('said', 'able'), ('said', 'ought'), ('said', 'master'), ('said', 'glad'), ('said', 'talk'), ('said', 'bad'), ('said', 'ca'), ('said', 'need'), ('said', 'wonder'), ('said', 'wanted'), ('said', 'added'), ('said', 'pleased'), ('said', 'wrong'), ('said', 'trouble'), ('said', 'command'), ('said', 'forget'), ('said', 'maam'), ('said', 'sorry'), ('said', 'dare'), ('said', 'meant'), ('said', 'dream'), ('said', 'aye'), ('said', 'consider'), ('said', 'knows'), ('unto', 'mr'), ('unto', 'also'), ('unto', 'saying'), ('unto', 'therefore'), ('unto', 'behold'), ('unto', 'thus'), ('unto', 'voice'), ('unto', 'hear'), ('unto', 'quite'), ('unto', 'oh'), ('unto', 'words'), ('unto', 'sir'), ('unto', 'indeed'), ('unto', 'answered'), ('unto', 'told'), ('unto', 'friend'), ('unto', 'dear'), ('unto', 'yes'), ('unto', 'call'), ('unto', 'asked'), ('unto', 'servant'), ('unto', 'servants'), ('unto', 'mean'), ('unto', 'true'), ('unto', 'wish'), ('unto', 'commanded'), ('unto', 'answer'), ('unto', 'ask'), ('unto', 'help'), ('unto', 'macian'), ('unto', 'remember'), ('unto', 'certainly'), ('unto', 'alice'), ('unto', 'wherefore'), ('unto', 'master'), ('unto', 'suppose')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c. Recall Errors\n",
        "non_stopword_words = [word for word in cbow_model.wv.index_to_key if word.lower() not in stopwords.words('english')][:1000]\n",
        "threshold = 0.8\n",
        "recall_errors = []\n",
        "for word_pair in combinations(non_stopword_words, 2):\n",
        "  similarity_emb = cbow_model.wv.similarity(word_pair[0], word_pair[1])\n",
        "  word1_lemma = WordNetLemmatizer().lemmatize(word_pair[0])\n",
        "  word2_lemma = WordNetLemmatizer().lemmatize(word_pair[1])\n",
        "  synsets_word1 = set(wn.synsets(word1_lemma))\n",
        "  synsets_word2 = set(wn.synsets(word2_lemma))\n",
        "  common_synsets = synsets_word1.intersection(synsets_word2)\n",
        "  similarity_wn = len(common_synsets)\n",
        "  if similarity_emb < threshold and similarity_wn > 0:\n",
        "    recall_errors.append(word_pair)\n",
        "print(\"Recall Errors:\", recall_errors[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HdbOAzfSCmw",
        "outputId": "d29bcbcc-288b-45c9-b648-9735eb3f01a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recall Errors: [('said', 'told'), ('said', 'read'), ('said', 'state'), ('said', 'order'), ('thou', 'thousand'), ('man', 'men'), ('man', 'world'), ('man', 'gentleman'), ('man', 'human'), ('man', 'gentlemen'), ('man', 'pieces'), ('god', 'gods'), ('come', 'came'), ('come', 'done'), ('come', 'fell'), ('come', 'coming'), ('come', 'got'), ('come', 'seed'), ('come', 'comes'), ('come', 'followed'), ('come', 'number'), ('come', 'follow'), ('come', 'fallen'), ('like', 'wish'), ('like', 'care'), ('came', 'done'), ('came', 'get'), ('came', 'coming'), ('came', 'got'), ('came', 'fall'), ('came', 'comes'), ('came', 'followed'), ('came', 'number'), ('came', 'follow'), ('came', 'fallen'), ('day', 'days'), ('king', 'power'), ('king', 'kings'), ('know', 'love'), ('know', 'knew'), ('know', 'known'), ('know', 'living'), ('know', 'bed'), ('know', 'lived'), ('know', 'loved'), ('know', 'knowing'), ('see', 'saw'), ('see', 'heard'), ('see', 'found'), ('see', 'looked'), ('see', 'seen'), ('see', 'looking'), ('see', 'meet'), ('see', 'seeing'), ('see', 'visit'), ('see', 'watch'), ('see', 'met'), ('see', 'understanding'), ('see', 'caught'), ('see', 'fancy'), ('see', 'figure'), ('see', 'looks'), ('see', 'witness'), ('see', 'regard'), ('see', 'determined'), ('see', 'view'), ('see', 'hearing'), ('go', 'went'), ('go', 'last'), ('go', 'work'), ('go', 'going'), ('go', 'gone'), ('go', 'live'), ('go', 'passed'), ('go', 'got'), ('go', 'living'), ('go', 'broken'), ('go', 'ran'), ('go', 'offer'), ('go', 'works'), ('go', 'sound'), ('go', 'run'), ('go', 'become'), ('go', 'died'), ('go', 'became'), ('go', 'lived'), ('go', 'break'), ('go', 'departed'), ('go', 'led'), ('go', 'moved'), ('go', 'goes'), ('go', 'running'), ('go', 'fit'), ('house', 'home'), ('house', 'family'), ('house', 'houses'), ('house', 'families'), ('house', 'sign'), ('people', 'multitude'), ('son', 'words')]\n"
          ]
        }
      ]
    }
  ]
}