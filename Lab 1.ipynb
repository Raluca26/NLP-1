{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbGSaGQ9vIS3"
      },
      "source": [
        "# NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07IDg6-UvIS6"
      },
      "source": [
        "#### Install NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGzlCrIivIS7",
        "outputId": "e904aaa0-5ca2-45c4-dec2-3b413452eeaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B58lHrhEvIS9"
      },
      "source": [
        "#### Download models or corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwQRlgElvIS9",
        "outputId": "ed07ae53-2b0e-4ba6-94cf-da104e866603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.9/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> Traceback (most recent call last):\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
            "    return _run_code(code, main_globals, None,\n",
            "  File \"/usr/lib/python3.9/runpy.py\", line 87, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nltk/downloader.py\", line 2550, in <module>\n",
            "    downloader.download(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nltk/downloader.py\", line 763, in download\n",
            "    self._interactive_download()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nltk/downloader.py\", line 1115, in _interactive_download\n",
            "    DownloaderShell(self).run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/nltk/downloader.py\", line 1141, in run\n",
            "    user_input = input(\"Downloader> \").strip()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# import nltk\n",
        "!python -m nltk.downloader # shows a window when graphical output available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvj0wdJmvIS-",
        "outputId": "b8dd41b1-b1af-4a50-87a0-b1ae9b8ccddb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQEsyB5JvITB"
      },
      "source": [
        "#### Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dN3HUcBEvIS_"
      },
      "outputs": [],
      "source": [
        "tweet = \"RT @lOR42wsOEFcv3f: I fall too fast, crash too hard, forgive too easily and care too much... :( #amiright\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kl8O_CcvITA"
      },
      "outputs": [],
      "source": [
        "query = 'fast'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFK_-15yvIS9"
      },
      "source": [
        "The naive way..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz3v_jj0vITA",
        "outputId": "e311c71e-15c7-45a9-990c-46cfa6c1bb7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "31"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "tweet.find(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xhFpKFpvITB",
        "outputId": "e6d31d1a-c8a4-472e-8d51-feac15f5bd3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '@lOR42wsOEFcv3f:',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast,',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard,',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much...',\n",
              " ':(',\n",
              " '#amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "tweet.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sx9xgcFvITC",
        "outputId": "76ac00ac-cdd1-493e-e058-ecd05f486bc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[False]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "[query in tweet.split()]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correct tokenization: informed splitting of the text into tokens"
      ],
      "metadata": {
        "id": "gjhBVUZK1LV4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoieLiJcvITC",
        "outputId": "7d8f3b28-e08b-4b69-8a34-1c746a91c401"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '@',\n",
              " 'lOR42wsOEFcv3f',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':',\n",
              " '(',\n",
              " '#',\n",
              " 'amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "\n",
        "nltk.word_tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA6xDrMdvITC",
        "outputId": "07f81234-48a8-4d35-d6d6-e00d7785dfc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "[query in nltk.word_tokenize(tweet)]\n",
        "# query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA5qgGixvITD",
        "outputId": "3a74d9bc-a076-4634-f43a-73926e2d8a7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '@',\n",
              " 'lOR42wsOEFcv3f',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':',\n",
              " '(',\n",
              " '#',\n",
              " 'amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "nltk.word_tokenize(tweet, language='spanish')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More options..."
      ],
      "metadata": {
        "id": "-KBibnjA1b26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDcB89vWvITD"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "custom_tokenizer = RegexpTokenizer('[a-zA-Z0-9]*', discard_empty=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbfG3liKvITE",
        "outputId": "6d5cc1f2-e167-4583-bdbf-a4a09f8a0744"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " '',\n",
              " '',\n",
              " 'lOR42wsOEFcv3f',\n",
              " '',\n",
              " '',\n",
              " 'I',\n",
              " '',\n",
              " 'fall',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'fast',\n",
              " '',\n",
              " '',\n",
              " 'crash',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'hard',\n",
              " '',\n",
              " '',\n",
              " 'forgive',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'easily',\n",
              " '',\n",
              " 'and',\n",
              " '',\n",
              " 'care',\n",
              " '',\n",
              " 'too',\n",
              " '',\n",
              " 'much',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " '',\n",
              " 'amiright',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "custom_tokenizer.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUojQmtRvITE"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm8kis0bvITE",
        "outputId": "5a31e1e7-c15e-46a1-f8b0-4609d7acd3a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(',\n",
              " '#amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "tweet_tokenizer.tokenize(tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yltAQxCvITE",
        "outputId": "ad3ca6a6-7ec5-47ef-fcb4-fb59c6528250"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT',\n",
              " ':',\n",
              " 'I',\n",
              " 'fall',\n",
              " 'too_fast',\n",
              " ',',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " ',',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much',\n",
              " '...',\n",
              " ':(',\n",
              " '#amiright']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "mwe = MWETokenizer()\n",
        "mwe.add_mwe(('too', 'fast'))\n",
        "mwe.tokenize(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fDVPDRnvITF"
      },
      "outputs": [],
      "source": [
        "mwe.add_mwe((('too', 'fast'), ('too', 'hard')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugIC2npavITF",
        "outputId": "9d48882f-e32b-435d-ed45-2a6962fe8dbe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "query = 'fast'\n",
        "query in mwe.tokenize(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj-5pXTyvITG"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bRK61fGevITG",
        "outputId": "c8932b77-28dc-42bd-d914-756387399b17"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rt @lor42wsoefcv3f: i fall too fast, crash too hard, forgive too easily and care too much... :( #amiright'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "tweet.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4-6Rnr7vITG"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def normalize_tokens(tokenized_text):\n",
        "    # Lowercase\n",
        "    tokens = [t.lower() for t in tokenized_text]\n",
        "    # Remove hashtags\n",
        "    tokens = [t for t in tokens if not t.startswith('#')]\n",
        "    # Remove punctuation\n",
        "    tokens = [t for t in tokens if t not in string.punctuation]\n",
        "    # Keep only letters\n",
        "    tokens = [t for t in tokens if re.match('^[a-z]+$', t)]\n",
        "    # Normalize characters\n",
        "    tokens = [re.sub('á', 'a', t) for t in tokens]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edRDrifZvITG",
        "outputId": "0b01b524-c771-4cdf-dc47-c748f9d84d05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['muy']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "spanish_query = 'muy rápido'\n",
        "normalize_tokens(tweet_tokenizer.tokenize(spanish_query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "HFpAR4b_vITH",
        "outputId": "edd47fcc-39bb-4160-a086-ece88f5408fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'muy rapido'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "!pip install unidecode\n",
        "import unidecode\n",
        "unidecode.unidecode(spanish_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR5AUDgevITH",
        "outputId": "769c3697-53b0-4add-fb34-6dce7f1c9d22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "normalize_tokens(tweet_tokenizer.tokenize(tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLG3Pd4SvITH"
      },
      "source": [
        "#### Uniform normalization principle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "491ipWWuvITH",
        "outputId": "6271710e-0fd7-4e53-e648-b741a9b3a8e4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['too', 'fast', 'too', 'furious']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "query = 'TOO fast TOO furious'\n",
        "tokenized_query = tweet_tokenizer.tokenize(query)\n",
        "normalized_query = normalize_tokens(tokenized_query)\n",
        "# normalized_query = tokenized_query\n",
        "normalized_query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pfpl7C7nvITH",
        "outputId": "8d1417cf-2fc3-4071-e1b2-4ec4d4b07025"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "# normalized_tweet = normalize_tokens(tweet.split())\n",
        "normalized_tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "truJmsHpvITI",
        "outputId": "76a55e0f-a9b8-4c7e-8767-95ddd1a71b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fast', 'too'}\n",
            "2 common word(s)\n"
          ]
        }
      ],
      "source": [
        "common_words = set(normalized_query).intersection(normalized_tweet)\n",
        "print(common_words)\n",
        "print(len(common_words), \"common word(s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaLxOlOMvITJ"
      },
      "source": [
        "#### Stemming / Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pmCSIy4vITJ"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoXs3q-SvITJ",
        "outputId": "90a2959d-c476-41ad-8b65-37699456f653"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgiv',\n",
              " 'too',\n",
              " 'easili',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "[stemmer.stem(t) for t in normalized_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = nltk.LancasterStemmer() # is prone to overstemming\n",
        "[stemmer.stem(t) for t in normalized_tweet]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mWZSR5d4OuH",
        "outputId": "922c1d7d-e1f6-4858-c746-c64e1f3b022b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fal',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forg',\n",
              " 'too',\n",
              " 'easy',\n",
              " 'and',\n",
              " 'car',\n",
              " 'too',\n",
              " 'much']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xxv9vv3TvITK",
        "outputId": "5113dcb4-e01f-4e20-f934-82710659b6c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgiv',\n",
              " 'too',\n",
              " 'easili',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "stemmer = SnowballStemmer(language='english') # Porter2\n",
        "\n",
        "[stemmer.stem(t) for t in normalized_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stemmer.stem(\"running\"))\n",
        "\n",
        "print(stemmer.stem(\"runs\"))\n",
        "\n",
        "print(stemmer.stem(\"ran\"))\n",
        "\n",
        "print(stemmer.stem(\"darling\"))\n",
        "\n",
        "print(stemmer.stem(\"are\"))\n",
        "\n",
        "print(stemmer.stem(\"bring\"))\n",
        "\n",
        "print(stemmer.stem(\"being\"))\n",
        "\n",
        "print(stemmer.stem(\"Charles\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEDgnkOE4pza",
        "outputId": "28e7461f-dc85-46ba-fa35-77c9b0cd48e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "run\n",
            "ran\n",
            "darl\n",
            "are\n",
            "bring\n",
            "be\n",
            "charl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1caMRHT7vITK",
        "outputId": "afa209a3-12e7-4022-e9d0-b781446a12b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rt',\n",
              " 'i',\n",
              " 'fall',\n",
              " 'too',\n",
              " 'fast',\n",
              " 'crash',\n",
              " 'too',\n",
              " 'hard',\n",
              " 'forgive',\n",
              " 'too',\n",
              " 'easily',\n",
              " 'and',\n",
              " 'care',\n",
              " 'too',\n",
              " 'much']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "[lemmatizer.lemmatize(t) for t in normalized_tweet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9yhe4EZvITK",
        "outputId": "8a49efea-b816-47ab-ea11-994fde5f0c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('rt', 'NN'), ('i', 'NN'), ('fall', 'VBP'), ('too', 'RB'), ('fast', 'JJ'), ('crash', 'NN'), ('too', 'RB'), ('hard', 'JJ'), ('forgive', 'JJ'), ('too', 'RB'), ('easily', 'RB'), ('and', 'CC'), ('care', 'VB'), ('too', 'RB'), ('much', 'JJ')]\n"
          ]
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "tagged_tweet = nltk.pos_tag(normalized_tweet) # More on this next lab...\n",
        "print(tagged_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yww_9WdPvITK"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "tag_map = {'J': wn.ADJ, 'V': wn.VERB, 'R': wn.ADV, 'N': wn.NOUN}\n",
        "def get_lemmas(tokenized_text):\n",
        "    tagged_text = nltk.pos_tag(tokenized_text)\n",
        "    return [lemmatizer.lemmatize(w, pos=tag_map.get(p[0], wn.NOUN)) for (w, p) in tagged_text]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwkQvd-SvITL",
        "outputId": "dedd1137-22ef-4bf1-a154-6b14945973da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'fastest']\n"
          ]
        }
      ],
      "source": [
        "query = \"the fastest!\"\n",
        "normalized_query = normalize_tokens(tweet_tokenizer.tokenize(query))\n",
        "print(normalized_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8AjyEA5vITL",
        "outputId": "5f94524f-7c44-4e2e-a575-b728675651a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['rt', 'i', 'fall', 'too', 'fast', 'crash', 'too', 'hard', 'forgive', 'too', 'easily', 'and', 'care', 'too', 'much']\n",
            "['the', 'fast']\n"
          ]
        }
      ],
      "source": [
        "lemmatized_tweet = get_lemmas(normalized_tweet)\n",
        "lemmatized_query = get_lemmas(normalized_query)\n",
        "print(lemmatized_tweet)\n",
        "print(lemmatized_query)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R08IvRkUvITL",
        "outputId": "d198aa90-f9a0-4a99-a321-97e15a5ccc87"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'so', 'fast', 'i', 'am', 'the', 'fastest']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "tweet = \"I am so fast, I am the fastest!\"\n",
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "normalized_tweet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[lemmatizer.lemmatize(t) for t in normalized_tweet]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P9ov1TJ67Ml",
        "outputId": "5fcc821a-03ab-4be6-9d4f-f31bb806d319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'so', 'fast', 'i', 'am', 'the', 'fastest']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24QRJbnIvITM",
        "outputId": "1c61ae28-63ea-4706-a6bc-a9b199ecb152"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'be', 'so', 'fast', 'i', 'be', 'the', 'fast']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "get_lemmas(normalized_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq3JncFSvITL",
        "outputId": "b98fce56-e120-40ff-d1ed-b717c7d32efd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common words: {'fast'}\n"
          ]
        }
      ],
      "source": [
        "print(\"Common words:\", set(lemmatized_tweet).intersection(set(lemmatized_query)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exJwCbhSvITI"
      },
      "source": [
        "#### Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R7abjAkvITI",
        "outputId": "75c9ea0d-3138-4cc0-d1e5-db490ec50f24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu8sWb4vvITI",
        "outputId": "d24bd003-c545-471f-de5c-3a9baacc970f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNzURg2kvITJ"
      },
      "outputs": [],
      "source": [
        "blacklist_words = stopwords.words('english') + ['rt']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-0g081PvITJ",
        "outputId": "580732bd-6e4d-4cb4-f982-746cb1983de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fast', 'fastest']\n"
          ]
        }
      ],
      "source": [
        "cleaned_tweet = [t for t in normalized_tweet if t not in blacklist_words]\n",
        "print(cleaned_tweet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnD0SkZNvITM"
      },
      "source": [
        "#### Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xckCNborvITM",
        "outputId": "ba78343b-adfb-4a9f-898f-77866e623814"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 2), ('be', 2), ('fast', 2), ('so', 1), ('the', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "Counter(get_lemmas(normalized_tweet)).most_common(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOLFMVSFvITM",
        "outputId": "138405d1-2550-48c3-e570-6d4a6cdee17c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'be', 'so', 'fast', 'i', 'be', 'the', 'fast']\n"
          ]
        }
      ],
      "source": [
        "tweet = \"I am so fast, I am the fastest!\"\n",
        "normalized_tweet = normalize_tokens(tweet_tokenizer.tokenize(tweet))\n",
        "lemmatized_tweet = get_lemmas(normalized_tweet)\n",
        "print(lemmatized_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dR5W_IeqvITM",
        "outputId": "10e15ef0-f4d8-4b14-fb9b-b9287b73e8c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'i': 2, 'am': 2, 'so': 1, 'fast': 1, 'the': 1, 'fastest': 1})\n",
            "Counter({'i': 2, 'be': 2, 'fast': 2, 'so': 1, 'the': 1})\n"
          ]
        }
      ],
      "source": [
        "print(Counter(normalized_tweet))\n",
        "print(Counter(lemmatized_tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VIE2dhCvITM"
      },
      "source": [
        "#### Sentence segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDhiS2VGvITN"
      },
      "outputs": [],
      "source": [
        "query = \"I am too fast. I am too furious.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPADLll5vITN"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Pp_45fMvITN",
        "outputId": "c1fe6ee2-363d-46aa-cd3e-2fce0bfcf035"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I am too fast.', 'I am too furious.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "sent_tokenize(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rrVvihFvITN",
        "outputId": "e951fd46-0098-44a7-afa9-4666068fbc88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Soy muy rápido!', 'Estoy muy furioso!']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
        "spanish_query = 'Soy muy rápido! Estoy muy furioso!'\n",
        "spanish_tokenizer.tokenize(spanish_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PuN1DjvvITN",
        "outputId": "2a96d3dc-7473-45cf-fda6-ec0c0b12ada6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J.K. Rowling is rich.', 'I am not as rich as J.K.']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "sent_tokenize(\"J.K. Rowling is rich. I am not as rich as J.K.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJaD2GydvITO"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "PunktSentenceTokenizer??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mcm9gIBvITO"
      },
      "source": [
        "#### Numeral conversion"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install word2number\n",
        "!pip install num2words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuRsqSRbzuDk",
        "outputId": "16018b4e-2897-4d35-d8c5-c2d8bd76dcae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: word2number\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=e4d5bbbfaef94378c2ff3296d760969fcfc83e6632e19958f21fe44b896d58ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/4a/5b/d2f2df5c344ddbecb8bea759872c207ea91d93f57fb54e816e\n",
            "Successfully built word2number\n",
            "Installing collected packages: word2number\n",
            "Successfully installed word2number-1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 KB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=39cd0eff6269d59d342a51a4d50ae6de024404d974ce8c695f9cad3f6ef40b0e\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, num2words\n",
            "Successfully installed docopt-0.6.2 num2words-0.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbn8ybBLvITO",
        "outputId": "49a4b416-f57a-49de-b472-1e5ee528cebd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "import word2number\n",
        "from word2number import w2n\n",
        "w2n.word_to_num(\"eleven\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2n.word_to_num(\"twenty three\")"
      ],
      "metadata": {
        "id": "vpA1EUb5z-CT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007c3fa2-0dd6-4463-f18f-87dff8064ae4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "couI08gLvITO"
      },
      "outputs": [],
      "source": [
        "from num2words import num2words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num2words(12)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G8h7XGcKJbw0",
        "outputId": "31d59e7f-9113-418f-c6c1-a1f0182f3db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'twelve'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num2words(101)\n"
      ],
      "metadata": {
        "id": "mjtdQ_n60B2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "378f94fa-0e4c-469a-e47a-b23dc7806584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one hundred and one'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num2words(2020)"
      ],
      "metadata": {
        "id": "YAK0RUY_0Dnn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d2a35d95-eac8-425b-b93f-e238473568fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'two thousand and twenty'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2n.word_to_num(\"Twelve o'clock!\")"
      ],
      "metadata": {
        "id": "wCvn9L_W7TDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2495d1b9-8016-4d9e-cfe1-119582c51888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise (1p)\n"
      ],
      "metadata": {
        "id": "lM4HWS8I_hUT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find a recent news article online.\n",
        "Read it in a python variable (input it manually or read from a file).\n",
        "\n",
        "Write a function that normalizes the text and splits it into tokens. Add flags to customize the different preprocessing choices (which stemmer/lemmatizer to use, whether to lowercase, whether to convert numbers, whether to remove stopwords, ...).\n",
        "\n",
        "Store the vocabulary of unique tokens found in the text.\n",
        "\n",
        "Compare the number of unique tokens (\"types\") with different preprocessing settings."
      ],
      "metadata": {
        "id": "7VJBKi1B_lZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = 'The cat (Felis catus) is a domestic species of small carnivorous mammal.[1][2] It is the only domesticated species in the family Felidae and is commonly referred to as the domestic cat or house cat to distinguish it from the wild members of the family.[4] Cats are commonly kept as house pets but can also be farm cats or feral cats; the feral cat ranges freely and avoids human contact.[5] Domestic cats are valued by humans for companionship and their ability to kill rodents. About 60 cat breeds are recognized by various cat registries.[6] The cat is similar in anatomy to the other felid species: it has a strong flexible body, quick reflexes, sharp teeth, and retractable claws adapted to killing small prey. Its night vision and sense of smell are well developed. Cat communication includes vocalizations like meowing, purring, trilling, hissing, growling, and grunting as well as cat-specific body language. Although the cat is a social species, it is a solitary hunter. As a predator, it is crepuscular, i.e. most active at dawn and dusk. It can hear sounds too faint or too high in frequency for human ears, such as those made by mice and other small mammals.[7] It also secretes and perceives pheromones.[8] '"
      ],
      "metadata": {
        "id": "ga37tHqo_jqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocesare(text, lowercase=True, remove_numbers=False, remove_stopwords=True, stemming=None, lemmatization=None, punctuation=True):\n",
        "\n",
        "    #remove text within paranthesis\n",
        "    text = re.sub(r'[\\(\\[\\{].*?[\\)\\]\\}]', '', text)\n",
        "\n",
        "    #tokenize words\n",
        "    text = nltk.word_tokenize(text, language='english')\n",
        "\n",
        "    #lowercase\n",
        "    if lowercase:\n",
        "        text = [t.lower() for t in text]\n",
        "\n",
        "    #remove punctuation\n",
        "    if punctuation:\n",
        "        text = [t for t in text if t not in string.punctuation]\n",
        "\n",
        "    # by default remove numbers is set to False as I wanted to keep the numbers in text and only delete the numbers betwen paranthesis from the text\n",
        "    #( ex: The cat (Felis catus) is a domestic species of small carnivorous mammal.[1][2])\n",
        "    if remove_numbers:\n",
        "        text = [t for t in text if re.match('^[a-z]+$', t)]\n",
        "\n",
        "    #remove stopwords\n",
        "    if remove_stopwords:\n",
        "        text = [t for t in text if t not in blacklist_words]\n",
        "\n",
        "    #choose stemmer\n",
        "    if stemming:\n",
        "        if stemming == 'porter':\n",
        "            stemmer = PorterStemmer()\n",
        "        elif stemming == 'snowball':\n",
        "            stemmer = SnowballStemmer('english')\n",
        "        else:\n",
        "            raise ValueError(\"Choose valid stemmer!\")\n",
        "        text = [stemmer.stem(t) for t in text]\n",
        "\n",
        "    #choose lemmatizer\n",
        "    if lemmatization:\n",
        "        if lemmatization == 'wordnet':\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "        else:\n",
        "            raise ValueError(\"Choose valid lemmatizer!\")\n",
        "        text = [lemmatizer.lemmatize(t) for t in text]\n",
        "\n",
        "\n",
        "\n",
        "    #remove punctuation\n",
        "    if punctuation:\n",
        "        text = [t for t in text if t not in string.punctuation]\n",
        "\n",
        "    # by default remove numbers is set to False as I wanted to keep the numbers in text and only delete the numbers betwen paranthesis from the text\n",
        "    #( ex: The cat (Felis catus) is a domestic species of small carnivorous mammal.[1][2])\n",
        "    if remove_numbers:\n",
        "        text = [t for t in text if re.match('^[a-z]+$', t)]\n",
        "\n",
        "    #remove stopwords\n",
        "    if remove_stopwords:\n",
        "        text = [t for t in text if t not in blacklist_words]\n",
        "\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "hxMRcpFnoKj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_text_1 = preprocesare(raw_text, stemming= 'porter')\n",
        "preprocessed_text_2 = preprocesare(raw_text, lemmatization='wordnet', remove_numbers=True)\n",
        "preprocessed_text_3 = preprocesare(raw_text, stemming= 'porter', remove_stopwords=False)\n",
        "\n",
        "print(f\"Tokens length with stemming and default settings: {len(preprocessed_text_1)}\")\n",
        "print(f\"Tokens length with lemmatization and removing numbers: {len(preprocessed_text_2)}\")\n",
        "print(f\"Tokens length with stemming, removing numbers and keeping stopwords: {len(preprocessed_text_3)}\")"
      ],
      "metadata": {
        "id": "2X5EF1VmpSGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "611e6d01-67a4-4e5a-dc5e-35951325b554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens length with stemming and default settings: 118\n",
            "Tokens length with lemmatization and removing numbers: 115\n",
            "Tokens length with stemming, removing numbers and keeping stopwords: 203\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}